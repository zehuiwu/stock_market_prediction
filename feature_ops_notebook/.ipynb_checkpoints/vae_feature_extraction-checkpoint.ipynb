{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bba0835c-68a0-4f8c-8ee1-e251ceb642a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ac57c3cb-aa7b-4caf-ab52-942e52f45671",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 91\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b930b-40a4-46a7-b153-26a6aa048e47",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5dfceead-b330-4cfe-aee6-0be7cd1c2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ['TSLA', 'AAPL', 'AMZN', 'GOOG','NFLX', 'FB']\n",
    "train_df_list = []\n",
    "test_df_list = []\n",
    "\n",
    "for symbol in symbols:\n",
    "    # read in all features for each stock\n",
    "    df = pd.read_csv('C:/Users/KingO/JupyterProject/stock-market-prediction/data/eod/{}_features.csv'.format(symbol))\n",
    "    \n",
    "    # drop non-feature columns\n",
    "    df = df.drop(['Adj Close','Unnamed: 0','Date'],axis=1)\n",
    "    \n",
    "    # extract train/test set\n",
    "    train_df = df.iloc[0:400]\n",
    "    test_df = df.iloc[400:]\n",
    "    \n",
    "    # standardize\n",
    "    sc = StandardScaler()\n",
    "    train_df = sc.fit_transform(train_df)\n",
    "    test_df = sc.transform(test_df)\n",
    "    \n",
    "    # append to the list\n",
    "    train_df_list.append(pd.DataFrame(train_df))\n",
    "    test_df_list.append(pd.DataFrame(test_df))\n",
    "    \n",
    "# concat values for all 6 stocks\n",
    "train_df = pd.concat(train_df_list, ignore_index=True)    \n",
    "test_df = pd.concat(test_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "42abc3fd-41fe-46fb-9889-ce12ce4d12b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.958017</td>\n",
       "      <td>1.884023</td>\n",
       "      <td>1.916705</td>\n",
       "      <td>1.901026</td>\n",
       "      <td>-0.798851</td>\n",
       "      <td>1.333651</td>\n",
       "      <td>1.197181</td>\n",
       "      <td>0.167872</td>\n",
       "      <td>0.402061</td>\n",
       "      <td>-0.973941</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.628809</td>\n",
       "      <td>-1.013224</td>\n",
       "      <td>1.012107</td>\n",
       "      <td>-0.038434</td>\n",
       "      <td>0.100435</td>\n",
       "      <td>-0.268836</td>\n",
       "      <td>1.636687</td>\n",
       "      <td>-0.189240</td>\n",
       "      <td>-0.169629</td>\n",
       "      <td>1.901026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.801378</td>\n",
       "      <td>1.849247</td>\n",
       "      <td>1.869552</td>\n",
       "      <td>1.834544</td>\n",
       "      <td>-0.910362</td>\n",
       "      <td>1.326791</td>\n",
       "      <td>1.174901</td>\n",
       "      <td>0.445683</td>\n",
       "      <td>0.201143</td>\n",
       "      <td>-0.823475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.563350</td>\n",
       "      <td>-0.932629</td>\n",
       "      <td>0.981050</td>\n",
       "      <td>-0.287451</td>\n",
       "      <td>0.019186</td>\n",
       "      <td>-0.648472</td>\n",
       "      <td>1.646119</td>\n",
       "      <td>-0.496178</td>\n",
       "      <td>-0.480710</td>\n",
       "      <td>1.834544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.852219</td>\n",
       "      <td>1.793482</td>\n",
       "      <td>1.875730</td>\n",
       "      <td>1.777668</td>\n",
       "      <td>-0.987539</td>\n",
       "      <td>1.300404</td>\n",
       "      <td>1.155608</td>\n",
       "      <td>0.294064</td>\n",
       "      <td>0.052620</td>\n",
       "      <td>-0.391106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537686</td>\n",
       "      <td>-0.862649</td>\n",
       "      <td>0.854258</td>\n",
       "      <td>-0.563564</td>\n",
       "      <td>-0.109070</td>\n",
       "      <td>-0.998388</td>\n",
       "      <td>1.656861</td>\n",
       "      <td>-0.448939</td>\n",
       "      <td>-0.432576</td>\n",
       "      <td>1.777668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.164197</td>\n",
       "      <td>2.283512</td>\n",
       "      <td>2.225198</td>\n",
       "      <td>2.377473</td>\n",
       "      <td>-0.214228</td>\n",
       "      <td>1.377355</td>\n",
       "      <td>1.204833</td>\n",
       "      <td>1.122826</td>\n",
       "      <td>1.815126</td>\n",
       "      <td>5.772366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271023</td>\n",
       "      <td>-0.749475</td>\n",
       "      <td>1.370198</td>\n",
       "      <td>-0.046352</td>\n",
       "      <td>-0.093183</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>1.795681</td>\n",
       "      <td>3.185568</td>\n",
       "      <td>3.014731</td>\n",
       "      <td>2.377473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.339687</td>\n",
       "      <td>2.312033</td>\n",
       "      <td>2.169464</td>\n",
       "      <td>2.166954</td>\n",
       "      <td>-0.259295</td>\n",
       "      <td>1.348359</td>\n",
       "      <td>1.157353</td>\n",
       "      <td>0.641042</td>\n",
       "      <td>0.886897</td>\n",
       "      <td>-0.252082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157184</td>\n",
       "      <td>-0.634521</td>\n",
       "      <td>1.391456</td>\n",
       "      <td>0.300834</td>\n",
       "      <td>-0.000933</td>\n",
       "      <td>0.643031</td>\n",
       "      <td>1.850582</td>\n",
       "      <td>-1.165201</td>\n",
       "      <td>-1.172660</td>\n",
       "      <td>2.166954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>-2.759834</td>\n",
       "      <td>-2.751107</td>\n",
       "      <td>-2.691890</td>\n",
       "      <td>-2.655658</td>\n",
       "      <td>0.968510</td>\n",
       "      <td>-7.119914</td>\n",
       "      <td>-4.541069</td>\n",
       "      <td>-1.926691</td>\n",
       "      <td>-2.714726</td>\n",
       "      <td>-0.390151</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.697936</td>\n",
       "      <td>-1.636330</td>\n",
       "      <td>-3.373492</td>\n",
       "      <td>0.305985</td>\n",
       "      <td>-0.730631</td>\n",
       "      <td>1.586699</td>\n",
       "      <td>-2.277776</td>\n",
       "      <td>0.681081</td>\n",
       "      <td>0.687372</td>\n",
       "      <td>-2.655658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>-2.666568</td>\n",
       "      <td>-2.750867</td>\n",
       "      <td>-2.743485</td>\n",
       "      <td>-2.799859</td>\n",
       "      <td>1.229928</td>\n",
       "      <td>-7.587438</td>\n",
       "      <td>-4.801968</td>\n",
       "      <td>-2.654281</td>\n",
       "      <td>-2.870218</td>\n",
       "      <td>-0.178736</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.112740</td>\n",
       "      <td>-1.986634</td>\n",
       "      <td>-3.633472</td>\n",
       "      <td>0.354220</td>\n",
       "      <td>-0.493030</td>\n",
       "      <td>1.336650</td>\n",
       "      <td>-2.365808</td>\n",
       "      <td>-1.594662</td>\n",
       "      <td>-1.612975</td>\n",
       "      <td>-2.799859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>-2.958324</td>\n",
       "      <td>-2.894105</td>\n",
       "      <td>-2.994289</td>\n",
       "      <td>-2.943105</td>\n",
       "      <td>5.397213</td>\n",
       "      <td>-7.630095</td>\n",
       "      <td>-5.385961</td>\n",
       "      <td>-2.847828</td>\n",
       "      <td>-3.640698</td>\n",
       "      <td>-0.769390</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.574577</td>\n",
       "      <td>-2.366974</td>\n",
       "      <td>-3.948305</td>\n",
       "      <td>1.486670</td>\n",
       "      <td>-0.015399</td>\n",
       "      <td>2.697936</td>\n",
       "      <td>-2.471047</td>\n",
       "      <td>-1.635346</td>\n",
       "      <td>-1.655128</td>\n",
       "      <td>-2.943105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>-2.277002</td>\n",
       "      <td>-2.239713</td>\n",
       "      <td>-2.423410</td>\n",
       "      <td>-2.208255</td>\n",
       "      <td>8.558830</td>\n",
       "      <td>-6.099437</td>\n",
       "      <td>-4.556845</td>\n",
       "      <td>-1.367896</td>\n",
       "      <td>5.290718</td>\n",
       "      <td>1.982609</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.107119</td>\n",
       "      <td>-2.569930</td>\n",
       "      <td>-2.089025</td>\n",
       "      <td>2.812003</td>\n",
       "      <td>0.703233</td>\n",
       "      <td>4.064725</td>\n",
       "      <td>-2.465781</td>\n",
       "      <td>8.289657</td>\n",
       "      <td>7.658539</td>\n",
       "      <td>-2.208255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>-2.240173</td>\n",
       "      <td>-2.145099</td>\n",
       "      <td>-2.256207</td>\n",
       "      <td>-2.333834</td>\n",
       "      <td>3.073177</td>\n",
       "      <td>-7.155868</td>\n",
       "      <td>-4.960653</td>\n",
       "      <td>-1.630004</td>\n",
       "      <td>3.810426</td>\n",
       "      <td>0.670940</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.847057</td>\n",
       "      <td>-2.675931</td>\n",
       "      <td>-1.072755</td>\n",
       "      <td>2.629458</td>\n",
       "      <td>1.231788</td>\n",
       "      <td>2.986735</td>\n",
       "      <td>-2.462473</td>\n",
       "      <td>-1.275033</td>\n",
       "      <td>-1.283098</td>\n",
       "      <td>-2.333834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    1.958017  1.884023  1.916705  1.901026 -0.798851  1.333651  1.197181   \n",
       "1    1.801378  1.849247  1.869552  1.834544 -0.910362  1.326791  1.174901   \n",
       "2    1.852219  1.793482  1.875730  1.777668 -0.987539  1.300404  1.155608   \n",
       "3    2.164197  2.283512  2.225198  2.377473 -0.214228  1.377355  1.204833   \n",
       "4    2.339687  2.312033  2.169464  2.166954 -0.259295  1.348359  1.157353   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "505 -2.759834 -2.751107 -2.691890 -2.655658  0.968510 -7.119914 -4.541069   \n",
       "506 -2.666568 -2.750867 -2.743485 -2.799859  1.229928 -7.587438 -4.801968   \n",
       "507 -2.958324 -2.894105 -2.994289 -2.943105  5.397213 -7.630095 -5.385961   \n",
       "508 -2.277002 -2.239713 -2.423410 -2.208255  8.558830 -6.099437 -4.556845   \n",
       "509 -2.240173 -2.145099 -2.256207 -2.333834  3.073177 -7.155868 -4.960653   \n",
       "\n",
       "           7         8         9   ...        81        82        83  \\\n",
       "0    0.167872  0.402061 -0.973941  ... -0.628809 -1.013224  1.012107   \n",
       "1    0.445683  0.201143 -0.823475  ... -0.563350 -0.932629  0.981050   \n",
       "2    0.294064  0.052620 -0.391106  ... -0.537686 -0.862649  0.854258   \n",
       "3    1.122826  1.815126  5.772366  ... -0.271023 -0.749475  1.370198   \n",
       "4    0.641042  0.886897 -0.252082  ... -0.157184 -0.634521  1.391456   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "505 -1.926691 -2.714726 -0.390151  ... -2.697936 -1.636330 -3.373492   \n",
       "506 -2.654281 -2.870218 -0.178736  ... -3.112740 -1.986634 -3.633472   \n",
       "507 -2.847828 -3.640698 -0.769390  ... -3.574577 -2.366974 -3.948305   \n",
       "508 -1.367896  5.290718  1.982609  ... -3.107119 -2.569930 -2.089025   \n",
       "509 -1.630004  3.810426  0.670940  ... -2.847057 -2.675931 -1.072755   \n",
       "\n",
       "           84        85        86        87        88        89        90  \n",
       "0   -0.038434  0.100435 -0.268836  1.636687 -0.189240 -0.169629  1.901026  \n",
       "1   -0.287451  0.019186 -0.648472  1.646119 -0.496178 -0.480710  1.834544  \n",
       "2   -0.563564 -0.109070 -0.998388  1.656861 -0.448939 -0.432576  1.777668  \n",
       "3   -0.046352 -0.093183  0.074600  1.795681  3.185568  3.014731  2.377473  \n",
       "4    0.300834 -0.000933  0.643031  1.850582 -1.165201 -1.172660  2.166954  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "505  0.305985 -0.730631  1.586699 -2.277776  0.681081  0.687372 -2.655658  \n",
       "506  0.354220 -0.493030  1.336650 -2.365808 -1.594662 -1.612975 -2.799859  \n",
       "507  1.486670 -0.015399  2.697936 -2.471047 -1.635346 -1.655128 -2.943105  \n",
       "508  2.812003  0.703233  4.064725 -2.465781  8.289657  7.658539 -2.208255  \n",
       "509  2.629458  1.231788  2.986735 -2.462473 -1.275033 -1.283098 -2.333834  \n",
       "\n",
       "[510 rows x 91 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "69b792a7-26e9-4d38-8db7-1dbb03793f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = torch.tensor(np.arange(len(train_df)).astype(np.float32))\n",
    "train = torch.tensor(train_df.values.astype(np.float32)) \n",
    "train_tensor = torch.utils.data.TensorDataset(train, train_target) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2da05dfb-cc8e-4bce-8771-60cd42e149b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = torch.tensor(np.arange(len(test_df)).astype(np.float32))\n",
    "test = torch.tensor(test_df.values.astype(np.float32)) \n",
    "test_tensor = torch.utils.data.TensorDataset(test, test_target) \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_tensor, batch_size = 128, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66f721-d2d3-471f-939b-165d7aae357d",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7472f02b-9c09-4c28-909e-92777eb326d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(91, 60)\n",
    "        self.linear2 = nn.Linear(60, latent_dims)\n",
    "        self.linear3 = nn.Linear(60, latent_dims)\n",
    "\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "        self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu =  self.linear2(x)\n",
    "        sigma = torch.exp(self.linear3(x))\n",
    "        z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f3dd19a9-611c-42b4-a06f-6a4578d2d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dims, 60)\n",
    "        self.linear2 = nn.Linear(60, 91)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.linear1(z))\n",
    "        z = self.linear2(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "37766dc0-fbd8-4ab8-9f76-7dd7af205f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989cf99-5c4d-40e8-a036-07e65302bee7",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1291ab6c-161a-4426-aa1b-bce0ac3ec3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer):\n",
    "    model.train()    \n",
    "    train_loss = 0\n",
    "\n",
    "    for x,y in train_loader:\n",
    "        # use gpu if available\n",
    "        x= x.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        x_hat = model(x)\n",
    "        loss = ((x - x_hat)**2).sum() + model.encoder.kl\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return train_loss/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e4a7e099-f17d-45d9-9d9b-d45d881acedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(test_loader, model):\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = 0\n",
    "    \n",
    "    for x,y in test_loader:\n",
    "        # use gpu if available\n",
    "        x= x.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        x_hat = model(x)\n",
    "        loss = ((x - x_hat)**2).sum() + model.encoder.kl\n",
    "        val_loss += loss.item()\n",
    "           \n",
    "    return val_loss/len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "024e1067-93af-4733-bb93-889bf2721172",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - train lose: 17876.42269736842 - test lose: 65091.6484375\n",
      "epoch 1 - train lose: 15788.037314967105 - test lose: 50114.751953125\n",
      "epoch 2 - train lose: 13674.828484786185 - test lose: 40785.181640625\n",
      "epoch 3 - train lose: 11390.066560444078 - test lose: 36374.498779296875\n",
      "epoch 4 - train lose: 10576.05363384046 - test lose: 35368.383544921875\n",
      "epoch 5 - train lose: 10164.822959498355 - test lose: 34038.60791015625\n",
      "epoch 6 - train lose: 9873.57527240954 - test lose: 33444.616455078125\n",
      "epoch 7 - train lose: 9690.862972861842 - test lose: 32680.033203125\n",
      "epoch 8 - train lose: 9596.693873355263 - test lose: 32433.473876953125\n",
      "epoch 9 - train lose: 9493.524311266447 - test lose: 32026.79345703125\n",
      "epoch 10 - train lose: 9416.061883223685 - test lose: 31495.2255859375\n",
      "epoch 11 - train lose: 9379.28019634046 - test lose: 30727.218994140625\n",
      "epoch 12 - train lose: 9260.63337787829 - test lose: 30537.554931640625\n",
      "epoch 13 - train lose: 9257.421566611842 - test lose: 29900.774169921875\n",
      "epoch 14 - train lose: 9169.933388157895 - test lose: 29752.5595703125\n",
      "epoch 15 - train lose: 9146.309775904605 - test lose: 29302.038330078125\n",
      "epoch 16 - train lose: 9026.034539473685 - test lose: 28720.6328125\n",
      "epoch 17 - train lose: 8991.489977384868 - test lose: 28452.338134765625\n",
      "epoch 18 - train lose: 8947.599737870065 - test lose: 28127.724365234375\n",
      "epoch 19 - train lose: 8895.691354851973 - test lose: 27924.08251953125\n",
      "epoch 20 - train lose: 8881.569670024672 - test lose: 27860.831787109375\n",
      "epoch 21 - train lose: 8796.771484375 - test lose: 27741.921875\n",
      "epoch 22 - train lose: 8761.35002055921 - test lose: 27700.5478515625\n",
      "epoch 23 - train lose: 8736.9482421875 - test lose: 27392.322509765625\n",
      "epoch 24 - train lose: 8687.98429790296 - test lose: 27237.49169921875\n",
      "epoch 25 - train lose: 8648.05119243421 - test lose: 27167.84912109375\n",
      "epoch 26 - train lose: 8594.139391447368 - test lose: 27046.99072265625\n",
      "epoch 27 - train lose: 8537.473787006578 - test lose: 26933.409912109375\n",
      "epoch 28 - train lose: 8511.201171875 - test lose: 26747.275146484375\n",
      "epoch 29 - train lose: 8472.69132915296 - test lose: 26626.31201171875\n",
      "epoch 30 - train lose: 8378.474146792763 - test lose: 26519.300048828125\n",
      "epoch 31 - train lose: 8357.051089638158 - test lose: 26457.1611328125\n",
      "epoch 32 - train lose: 8312.399953741777 - test lose: 26381.18798828125\n",
      "epoch 33 - train lose: 8248.664910567435 - test lose: 26099.3017578125\n",
      "epoch 34 - train lose: 8208.855828536185 - test lose: 26053.943603515625\n",
      "epoch 35 - train lose: 8180.1038240131575 - test lose: 25976.7041015625\n",
      "epoch 36 - train lose: 8124.416144120066 - test lose: 25844.31787109375\n",
      "epoch 37 - train lose: 8065.334549753289 - test lose: 25668.2353515625\n",
      "epoch 38 - train lose: 8068.324372944079 - test lose: 25534.450927734375\n",
      "epoch 39 - train lose: 8047.248972039473 - test lose: 25482.00244140625\n",
      "epoch 40 - train lose: 7985.346037212171 - test lose: 25431.428466796875\n",
      "epoch 41 - train lose: 7951.627158717105 - test lose: 25429.5576171875\n",
      "epoch 42 - train lose: 7923.427374588816 - test lose: 25134.174072265625\n",
      "epoch 43 - train lose: 7889.729774876645 - test lose: 25047.978759765625\n",
      "epoch 44 - train lose: 7869.0586965460525 - test lose: 24935.213134765625\n",
      "epoch 45 - train lose: 7825.2799650493425 - test lose: 24811.37060546875\n",
      "epoch 46 - train lose: 7785.650030838816 - test lose: 24622.538818359375\n",
      "epoch 47 - train lose: 7774.10791015625 - test lose: 24563.719970703125\n",
      "epoch 48 - train lose: 7719.433953536184 - test lose: 24517.0341796875\n",
      "epoch 49 - train lose: 7702.690995065789 - test lose: 24326.40869140625\n",
      "epoch 50 - train lose: 7676.4115182976975 - test lose: 24161.03125\n",
      "epoch 51 - train lose: 7661.0529142680925 - test lose: 24148.747802734375\n",
      "epoch 52 - train lose: 7624.4006990131575 - test lose: 24099.141357421875\n",
      "epoch 53 - train lose: 7611.447394120066 - test lose: 24112.101318359375\n",
      "epoch 54 - train lose: 7595.319849917763 - test lose: 23992.63818359375\n",
      "epoch 55 - train lose: 7551.6624948601975 - test lose: 23778.948486328125\n",
      "epoch 56 - train lose: 7545.217722039473 - test lose: 23890.90869140625\n",
      "epoch 57 - train lose: 7517.296900699013 - test lose: 23663.97607421875\n",
      "epoch 58 - train lose: 7498.921258223684 - test lose: 23731.547119140625\n",
      "epoch 59 - train lose: 7473.321648848684 - test lose: 23595.289306640625\n",
      "epoch 60 - train lose: 7466.944772820723 - test lose: 23482.655517578125\n",
      "epoch 61 - train lose: 7425.6560701069075 - test lose: 23490.7041015625\n",
      "epoch 62 - train lose: 7422.655787417763 - test lose: 23436.39990234375\n",
      "epoch 63 - train lose: 7417.2116056743425 - test lose: 23243.349609375\n",
      "epoch 64 - train lose: 7395.424085115132 - test lose: 23256.585693359375\n",
      "epoch 65 - train lose: 7386.202842310855 - test lose: 23240.24658203125\n",
      "epoch 66 - train lose: 7376.220086348684 - test lose: 23151.466552734375\n",
      "epoch 67 - train lose: 7351.4891550164475 - test lose: 23061.778076171875\n",
      "epoch 68 - train lose: 7354.138543379934 - test lose: 23056.633056640625\n",
      "epoch 69 - train lose: 7325.436497738487 - test lose: 22951.904296875\n",
      "epoch 70 - train lose: 7322.8748972039475 - test lose: 22863.484375\n",
      "epoch 71 - train lose: 7310.496633429277 - test lose: 22880.954345703125\n",
      "epoch 72 - train lose: 7304.558233963816 - test lose: 22712.877685546875\n",
      "epoch 73 - train lose: 7289.4154245476975 - test lose: 22778.18505859375\n",
      "epoch 74 - train lose: 7289.3136564555925 - test lose: 22644.935791015625\n",
      "epoch 75 - train lose: 7279.234606291118 - test lose: 22595.787109375\n",
      "epoch 76 - train lose: 7267.271227384868 - test lose: 22598.9697265625\n",
      "epoch 77 - train lose: 7268.187551398027 - test lose: 22620.41064453125\n",
      "epoch 78 - train lose: 7252.216796875 - test lose: 22672.16455078125\n",
      "epoch 79 - train lose: 7255.416452508223 - test lose: 22465.76025390625\n",
      "epoch 80 - train lose: 7250.1786081414475 - test lose: 22634.47119140625\n",
      "epoch 81 - train lose: 7241.767989309211 - test lose: 22520.490478515625\n",
      "epoch 82 - train lose: 7237.1618009868425 - test lose: 22486.3681640625\n",
      "epoch 83 - train lose: 7218.290193256579 - test lose: 22170.629638671875\n",
      "epoch 84 - train lose: 7222.446366159539 - test lose: 22191.427001953125\n",
      "epoch 85 - train lose: 7216.303453947368 - test lose: 22359.15966796875\n",
      "epoch 86 - train lose: 7213.687782689145 - test lose: 22222.75244140625\n",
      "epoch 87 - train lose: 7202.038445723684 - test lose: 22274.4033203125\n",
      "epoch 88 - train lose: 7207.276418585527 - test lose: 22222.63720703125\n",
      "epoch 89 - train lose: 7203.547517475329 - test lose: 22173.17919921875\n",
      "epoch 90 - train lose: 7186.488846628289 - test lose: 22103.86328125\n",
      "epoch 91 - train lose: 7189.985814144737 - test lose: 22138.585205078125\n",
      "epoch 92 - train lose: 7184.642706620066 - test lose: 22148.46484375\n",
      "epoch 93 - train lose: 7185.4095651726975 - test lose: 22050.9619140625\n",
      "epoch 94 - train lose: 7192.3073087993425 - test lose: 22002.489501953125\n",
      "epoch 95 - train lose: 7174.848478618421 - test lose: 21941.331787109375\n",
      "epoch 96 - train lose: 7167.333984375 - test lose: 21881.970947265625\n",
      "epoch 97 - train lose: 7169.524645353618 - test lose: 21937.7890625\n",
      "epoch 98 - train lose: 7183.1687654194075 - test lose: 22002.89208984375\n",
      "epoch 99 - train lose: 7167.139931126645 - test lose: 21825.145751953125\n",
      "epoch 100 - train lose: 7162.694490131579 - test lose: 21961.00927734375\n",
      "epoch 101 - train lose: 7171.38671875 - test lose: 21957.493896484375\n",
      "epoch 102 - train lose: 7162.878006784539 - test lose: 21653.349853515625\n",
      "epoch 103 - train lose: 7160.6546823601975 - test lose: 21728.37646484375\n",
      "epoch 104 - train lose: 7145.333059210527 - test lose: 21802.8623046875\n",
      "epoch 105 - train lose: 7155.263106496711 - test lose: 21985.63916015625\n",
      "epoch 106 - train lose: 7145.5695158305925 - test lose: 21797.478759765625\n",
      "epoch 107 - train lose: 7144.533280222039 - test lose: 21800.85205078125\n",
      "epoch 108 - train lose: 7145.4636872944075 - test lose: 21819.789794921875\n",
      "epoch 109 - train lose: 7134.853232935855 - test lose: 21712.583740234375\n",
      "epoch 110 - train lose: 7129.312294407895 - test lose: 21600.1875\n",
      "epoch 111 - train lose: 7118.915296052632 - test lose: 21646.25830078125\n",
      "epoch 112 - train lose: 7132.546926398027 - test lose: 21627.4541015625\n",
      "epoch 113 - train lose: 7110.132349917763 - test lose: 21580.767822265625\n",
      "epoch 114 - train lose: 7122.659950657895 - test lose: 21606.659912109375\n",
      "epoch 115 - train lose: 7111.263389185855 - test lose: 21563.679931640625\n",
      "epoch 116 - train lose: 7112.661878083882 - test lose: 21454.04931640625\n",
      "epoch 117 - train lose: 7107.5438425164475 - test lose: 21665.8251953125\n",
      "epoch 118 - train lose: 7107.685880962171 - test lose: 21737.73095703125\n",
      "epoch 119 - train lose: 7108.947702508223 - test lose: 21586.959716796875\n",
      "epoch 120 - train lose: 7092.112278988487 - test lose: 21554.95654296875\n",
      "epoch 121 - train lose: 7095.583264802632 - test lose: 21511.04052734375\n",
      "epoch 122 - train lose: 7097.20556640625 - test lose: 21524.507568359375\n",
      "epoch 123 - train lose: 7088.565018503289 - test lose: 21552.98583984375\n",
      "epoch 124 - train lose: 7081.805638363487 - test lose: 21388.26708984375\n",
      "epoch 125 - train lose: 7084.545255962171 - test lose: 21395.123291015625\n",
      "epoch 126 - train lose: 7092.250565378289 - test lose: 21409.36767578125\n",
      "epoch 127 - train lose: 7076.347887541118 - test lose: 21413.701904296875\n",
      "epoch 128 - train lose: 7078.450555098684 - test lose: 21313.831298828125\n",
      "epoch 129 - train lose: 7070.677657277961 - test lose: 21269.12744140625\n",
      "epoch 130 - train lose: 7066.938116776316 - test lose: 21261.669189453125\n",
      "epoch 131 - train lose: 7063.450760690789 - test lose: 21375.29833984375\n",
      "epoch 132 - train lose: 7065.761795847039 - test lose: 21373.0341796875\n",
      "epoch 133 - train lose: 7074.5443307976975 - test lose: 21157.40234375\n",
      "epoch 134 - train lose: 7061.271689967105 - test lose: 21263.534912109375\n",
      "epoch 135 - train lose: 7054.307745682566 - test lose: 21211.04736328125\n",
      "epoch 136 - train lose: 7040.230597245066 - test lose: 21274.8583984375\n",
      "epoch 137 - train lose: 7035.339509662829 - test lose: 21161.8369140625\n",
      "epoch 138 - train lose: 7027.586836965461 - test lose: 21062.594482421875\n",
      "epoch 139 - train lose: 7029.9593698601975 - test lose: 21142.6318359375\n",
      "epoch 140 - train lose: 7035.511693050987 - test lose: 21180.80517578125\n",
      "epoch 141 - train lose: 7023.6533203125 - test lose: 21169.35888671875\n",
      "epoch 142 - train lose: 7024.1717722039475 - test lose: 21062.81787109375\n",
      "epoch 143 - train lose: 7031.532072368421 - test lose: 21259.82470703125\n",
      "epoch 144 - train lose: 7024.186600534539 - test lose: 21005.62890625\n",
      "epoch 145 - train lose: 7020.470754523027 - test lose: 20948.876708984375\n",
      "epoch 146 - train lose: 7012.3322111430925 - test lose: 20953.429443359375\n",
      "epoch 147 - train lose: 7011.903834292763 - test lose: 20833.780517578125\n",
      "epoch 148 - train lose: 7005.080489309211 - test lose: 20832.165771484375\n",
      "epoch 149 - train lose: 7011.538548519737 - test lose: 21008.05126953125\n",
      "epoch 150 - train lose: 7013.546720805921 - test lose: 20790.648193359375\n",
      "epoch 151 - train lose: 7002.417120682566 - test lose: 21021.59033203125\n",
      "epoch 152 - train lose: 7000.0662263569075 - test lose: 20827.79248046875\n",
      "epoch 153 - train lose: 7011.0885587993425 - test lose: 20802.22119140625\n",
      "epoch 154 - train lose: 6991.905710320723 - test lose: 20840.61767578125\n",
      "epoch 155 - train lose: 6996.4932668585525 - test lose: 20857.58349609375\n",
      "epoch 156 - train lose: 6984.2638517680925 - test lose: 20794.647216796875\n",
      "epoch 157 - train lose: 6986.692948190789 - test lose: 20627.244873046875\n",
      "epoch 158 - train lose: 6991.9122121710525 - test lose: 20640.48486328125\n",
      "epoch 159 - train lose: 6977.635022615132 - test lose: 20829.819091796875\n",
      "epoch 160 - train lose: 6979.653731496711 - test lose: 20620.97265625\n",
      "epoch 161 - train lose: 6979.614334909539 - test lose: 20632.724853515625\n",
      "epoch 162 - train lose: 6997.4930612664475 - test lose: 20544.59814453125\n",
      "epoch 163 - train lose: 6973.039499383223 - test lose: 20667.710815429688\n",
      "epoch 164 - train lose: 6984.488563939145 - test lose: 20647.015625\n",
      "epoch 165 - train lose: 6973.862073396382 - test lose: 20602.47705078125\n",
      "epoch 166 - train lose: 6968.990774054277 - test lose: 20565.07421875\n",
      "epoch 167 - train lose: 6970.05810546875 - test lose: 20597.670166015625\n",
      "epoch 168 - train lose: 6970.010793585527 - test lose: 20464.681884765625\n",
      "epoch 169 - train lose: 6973.226896587171 - test lose: 20514.803344726562\n",
      "epoch 170 - train lose: 6971.449629934211 - test lose: 20524.67333984375\n",
      "epoch 171 - train lose: 6959.130114103618 - test lose: 20587.008544921875\n",
      "epoch 172 - train lose: 6963.150107935855 - test lose: 20540.637451171875\n",
      "epoch 173 - train lose: 6953.514802631579 - test lose: 20636.678466796875\n",
      "epoch 174 - train lose: 6954.025956003289 - test lose: 20690.646728515625\n",
      "epoch 175 - train lose: 6949.136847245066 - test lose: 20494.5712890625\n",
      "epoch 176 - train lose: 6944.8541837993425 - test lose: 20509.1904296875\n",
      "epoch 177 - train lose: 6951.892346833882 - test lose: 20500.041015625\n",
      "epoch 178 - train lose: 6949.038522820723 - test lose: 20461.253540039062\n",
      "epoch 179 - train lose: 6935.513723273027 - test lose: 20611.358764648438\n",
      "epoch 180 - train lose: 6946.5427888569075 - test lose: 20481.364990234375\n",
      "epoch 181 - train lose: 6938.5813630756575 - test lose: 20425.67724609375\n",
      "epoch 182 - train lose: 6936.306023848684 - test lose: 20615.727783203125\n",
      "epoch 183 - train lose: 6939.4382966694075 - test lose: 20383.81787109375\n",
      "epoch 184 - train lose: 6931.468313116777 - test lose: 20311.46044921875\n",
      "epoch 185 - train lose: 6935.504163240132 - test lose: 20461.875244140625\n",
      "epoch 186 - train lose: 6935.375950863487 - test lose: 20397.597290039062\n",
      "epoch 187 - train lose: 6937.2069284539475 - test lose: 20409.878662109375\n",
      "epoch 188 - train lose: 6924.466077302632 - test lose: 20288.67626953125\n",
      "epoch 189 - train lose: 6916.310906661184 - test lose: 20421.447387695312\n",
      "epoch 190 - train lose: 6931.438270970395 - test lose: 20368.580810546875\n",
      "epoch 191 - train lose: 6925.058927837171 - test lose: 20298.593505859375\n",
      "epoch 192 - train lose: 6914.585012335527 - test lose: 20430.441162109375\n",
      "epoch 193 - train lose: 6926.558825041118 - test lose: 20373.215576171875\n",
      "epoch 194 - train lose: 6930.400904605263 - test lose: 20382.541137695312\n",
      "epoch 195 - train lose: 6927.951403166118 - test lose: 20397.695068359375\n",
      "epoch 196 - train lose: 6920.9791837993425 - test lose: 20496.46240234375\n",
      "epoch 197 - train lose: 6915.0198396381575 - test lose: 20416.607055664062\n",
      "epoch 198 - train lose: 6925.8623046875 - test lose: 20317.327880859375\n",
      "epoch 199 - train lose: 6916.669896175987 - test lose: 20427.078125\n",
      "epoch 200 - train lose: 6929.604774876645 - test lose: 20303.88525390625\n",
      "epoch 201 - train lose: 6897.237150493421 - test lose: 20405.826782226562\n",
      "epoch 202 - train lose: 6910.729466488487 - test lose: 20434.936889648438\n",
      "epoch 203 - train lose: 6914.293868215461 - test lose: 20542.656616210938\n",
      "epoch 204 - train lose: 6903.954923930921 - test lose: 20296.7568359375\n",
      "epoch 205 - train lose: 6908.2520559210525 - test lose: 20343.5361328125\n",
      "epoch 206 - train lose: 6912.806435032895 - test lose: 20221.41796875\n",
      "epoch 207 - train lose: 6906.365696957237 - test lose: 20273.582885742188\n",
      "epoch 208 - train lose: 6906.127312911184 - test lose: 20339.181274414062\n",
      "epoch 209 - train lose: 6908.186163651316 - test lose: 20230.877197265625\n",
      "epoch 210 - train lose: 6900.5441252055925 - test lose: 20215.100708007812\n",
      "epoch 211 - train lose: 6902.7650339226975 - test lose: 20245.6240234375\n",
      "epoch 212 - train lose: 6896.463019120066 - test lose: 20166.992309570312\n",
      "epoch 213 - train lose: 6891.766447368421 - test lose: 20265.552001953125\n",
      "epoch 214 - train lose: 6893.166915090461 - test lose: 20307.839965820312\n",
      "epoch 215 - train lose: 6897.884020353618 - test lose: 20139.583740234375\n",
      "epoch 216 - train lose: 6889.574861225329 - test lose: 20160.032104492188\n",
      "epoch 217 - train lose: 6891.600534539473 - test lose: 20132.794189453125\n",
      "epoch 218 - train lose: 6899.965563322368 - test lose: 20200.86572265625\n",
      "epoch 219 - train lose: 6903.261101973684 - test lose: 20125.490234375\n",
      "epoch 220 - train lose: 6890.57177734375 - test lose: 20178.739013671875\n",
      "epoch 221 - train lose: 6887.691457648027 - test lose: 20172.49365234375\n",
      "epoch 222 - train lose: 6890.225997121711 - test lose: 20121.019165039062\n",
      "epoch 223 - train lose: 6887.307385896382 - test lose: 20140.524291992188\n",
      "epoch 224 - train lose: 6881.719084087171 - test lose: 20162.2451171875\n",
      "epoch 225 - train lose: 6885.957391036184 - test lose: 20249.783203125\n",
      "epoch 226 - train lose: 6872.6917146381575 - test lose: 20244.96044921875\n",
      "epoch 227 - train lose: 6890.854826274671 - test lose: 20115.684814453125\n",
      "epoch 228 - train lose: 6891.733809621711 - test lose: 20229.5244140625\n",
      "epoch 229 - train lose: 6873.042197779605 - test lose: 20032.545166015625\n",
      "epoch 230 - train lose: 6877.082288240132 - test lose: 20163.461181640625\n",
      "epoch 231 - train lose: 6873.2744140625 - test lose: 20336.762573242188\n",
      "epoch 232 - train lose: 6884.65087890625 - test lose: 20263.922607421875\n",
      "epoch 233 - train lose: 6874.972681949013 - test lose: 20124.8173828125\n",
      "epoch 234 - train lose: 6872.382452713816 - test lose: 20208.545532226562\n",
      "epoch 235 - train lose: 6868.835654810855 - test lose: 20115.36962890625\n",
      "epoch 236 - train lose: 6880.684904399671 - test lose: 20251.071533203125\n",
      "epoch 237 - train lose: 6871.464201274671 - test lose: 20292.209228515625\n",
      "epoch 238 - train lose: 6875.9530222039475 - test lose: 20094.872192382812\n",
      "epoch 239 - train lose: 6862.130730879934 - test lose: 20079.758666992188\n",
      "epoch 240 - train lose: 6868.700529399671 - test lose: 20049.853271484375\n",
      "epoch 241 - train lose: 6868.593981291118 - test lose: 20135.946411132812\n",
      "epoch 242 - train lose: 6855.735608552632 - test lose: 20020.841064453125\n",
      "epoch 243 - train lose: 6860.399825246711 - test lose: 20144.544189453125\n",
      "epoch 244 - train lose: 6867.127081620066 - test lose: 20106.462890625\n",
      "epoch 245 - train lose: 6869.053248355263 - test lose: 20030.655029296875\n",
      "epoch 246 - train lose: 6870.525596217105 - test lose: 19998.749267578125\n",
      "epoch 247 - train lose: 6861.249306126645 - test lose: 20022.438720703125\n",
      "epoch 248 - train lose: 6866.555124383223 - test lose: 20016.24365234375\n",
      "epoch 249 - train lose: 6865.334935238487 - test lose: 20253.311767578125\n",
      "epoch 250 - train lose: 6870.836836965461 - test lose: 20041.112182617188\n",
      "epoch 251 - train lose: 6857.713661595395 - test lose: 19987.505493164062\n",
      "epoch 252 - train lose: 6841.110582853618 - test lose: 19887.712646484375\n",
      "epoch 253 - train lose: 6854.58984375 - test lose: 20046.664672851562\n",
      "epoch 254 - train lose: 6848.0748098273025 - test lose: 19995.878051757812\n",
      "epoch 255 - train lose: 6856.032586348684 - test lose: 19984.1552734375\n",
      "epoch 256 - train lose: 6860.3040193256575 - test lose: 20056.443603515625\n",
      "epoch 257 - train lose: 6857.647101151316 - test lose: 19918.5068359375\n",
      "epoch 258 - train lose: 6848.326197574013 - test lose: 20227.46923828125\n",
      "epoch 259 - train lose: 6849.251130756579 - test lose: 20176.505615234375\n",
      "epoch 260 - train lose: 6843.526418585527 - test lose: 19950.728271484375\n",
      "epoch 261 - train lose: 6847.566560444079 - test lose: 20006.931884765625\n",
      "epoch 262 - train lose: 6846.4733244243425 - test lose: 20151.587280273438\n",
      "epoch 263 - train lose: 6846.912057976973 - test lose: 20004.363525390625\n",
      "epoch 264 - train lose: 6854.593672902961 - test lose: 20017.078491210938\n",
      "epoch 265 - train lose: 6855.486405222039 - test lose: 19927.16259765625\n",
      "epoch 266 - train lose: 6850.02490234375 - test lose: 19958.493408203125\n",
      "epoch 267 - train lose: 6850.0730622944075 - test lose: 20097.954467773438\n",
      "epoch 268 - train lose: 6841.816457648027 - test lose: 20060.978271484375\n",
      "epoch 269 - train lose: 6840.2373046875 - test lose: 20007.838500976562\n",
      "epoch 270 - train lose: 6844.2647512335525 - test lose: 19965.399536132812\n",
      "epoch 271 - train lose: 6849.9740182976975 - test lose: 20052.208374023438\n",
      "epoch 272 - train lose: 6832.710217927632 - test lose: 19928.239501953125\n",
      "epoch 273 - train lose: 6839.310623972039 - test lose: 20065.449462890625\n",
      "epoch 274 - train lose: 6840.517244037829 - test lose: 20039.687255859375\n",
      "epoch 275 - train lose: 6828.187422902961 - test lose: 19885.960083007812\n",
      "epoch 276 - train lose: 6828.342233758223 - test lose: 19995.9833984375\n",
      "epoch 277 - train lose: 6848.844777960527 - test lose: 20060.2451171875\n",
      "epoch 278 - train lose: 6832.100920024671 - test lose: 20193.099853515625\n",
      "epoch 279 - train lose: 6839.1124845805925 - test lose: 20101.001098632812\n",
      "epoch 280 - train lose: 6838.113306949013 - test lose: 20067.697143554688\n",
      "epoch 281 - train lose: 6839.329718338816 - test lose: 20037.087646484375\n",
      "epoch 282 - train lose: 6839.159796463816 - test lose: 20029.731201171875\n",
      "epoch 283 - train lose: 6836.173134251645 - test lose: 19960.61962890625\n",
      "epoch 284 - train lose: 6829.35888671875 - test lose: 19911.831787109375\n",
      "epoch 285 - train lose: 6843.3195158305925 - test lose: 19946.87109375\n",
      "epoch 286 - train lose: 6824.3119089226975 - test lose: 19969.685791015625\n",
      "epoch 287 - train lose: 6828.039396587171 - test lose: 19962.953125\n",
      "epoch 288 - train lose: 6833.908408717105 - test lose: 19920.842041015625\n",
      "epoch 289 - train lose: 6819.500822368421 - test lose: 19867.833618164062\n",
      "epoch 290 - train lose: 6835.305304276316 - test lose: 19726.123901367188\n",
      "epoch 291 - train lose: 6832.5753752055925 - test lose: 19812.741943359375\n",
      "epoch 292 - train lose: 6816.102564761513 - test lose: 19808.63720703125\n",
      "epoch 293 - train lose: 6830.658537212171 - test lose: 19792.791259765625\n",
      "epoch 294 - train lose: 6828.8694490131575 - test lose: 19757.006469726562\n",
      "epoch 295 - train lose: 6817.6796875 - test lose: 19810.037719726562\n",
      "epoch 296 - train lose: 6814.321032072368 - test lose: 19827.989379882812\n",
      "epoch 297 - train lose: 6837.813399465461 - test lose: 19934.600952148438\n",
      "epoch 298 - train lose: 6814.534899259868 - test lose: 19776.711181640625\n",
      "epoch 299 - train lose: 6815.8401521381575 - test lose: 20016.142578125\n",
      "epoch 300 - train lose: 6813.924650493421 - test lose: 19896.088256835938\n",
      "epoch 301 - train lose: 6824.2122224506575 - test lose: 19804.005004882812\n",
      "epoch 302 - train lose: 6823.046746504934 - test lose: 20076.85595703125\n",
      "epoch 303 - train lose: 6822.591180098684 - test lose: 20110.83349609375\n",
      "epoch 304 - train lose: 6832.744962993421 - test lose: 20001.052978515625\n",
      "epoch 305 - train lose: 6805.727616159539 - test lose: 19933.405517578125\n",
      "epoch 306 - train lose: 6809.367804276316 - test lose: 20112.242553710938\n",
      "epoch 307 - train lose: 6813.457365337171 - test lose: 20015.017211914062\n",
      "epoch 308 - train lose: 6817.659410978618 - test lose: 19878.098510742188\n",
      "epoch 309 - train lose: 6813.345446134868 - test lose: 19913.274658203125\n",
      "epoch 310 - train lose: 6812.144505550987 - test lose: 19913.513427734375\n",
      "epoch 311 - train lose: 6815.330489309211 - test lose: 19870.73388671875\n",
      "epoch 312 - train lose: 6818.2332956414475 - test lose: 19882.888916015625\n",
      "epoch 313 - train lose: 6823.597784745066 - test lose: 19795.171142578125\n",
      "epoch 314 - train lose: 6810.339098478618 - test lose: 19789.287109375\n",
      "epoch 315 - train lose: 6815.736456620066 - test lose: 19807.245239257812\n",
      "epoch 316 - train lose: 6810.013748972039 - test lose: 19766.222778320312\n",
      "epoch 317 - train lose: 6812.560778166118 - test lose: 19927.593383789062\n",
      "epoch 318 - train lose: 6802.653114720395 - test lose: 19884.651611328125\n",
      "epoch 319 - train lose: 6806.825940583882 - test lose: 19705.106689453125\n",
      "epoch 320 - train lose: 6811.206414473684 - test lose: 19879.394775390625\n",
      "epoch 321 - train lose: 6793.154065583882 - test lose: 19730.9228515625\n",
      "epoch 322 - train lose: 6803.9740182976975 - test lose: 19947.435791015625\n",
      "epoch 323 - train lose: 6813.770790501645 - test lose: 19850.50341796875\n",
      "epoch 324 - train lose: 6801.3790090460525 - test lose: 19655.32958984375\n",
      "epoch 325 - train lose: 6805.754857113487 - test lose: 19670.176635742188\n",
      "epoch 326 - train lose: 6807.2259714226975 - test lose: 19791.976684570312\n",
      "epoch 327 - train lose: 6795.207493832237 - test lose: 19740.318115234375\n",
      "epoch 328 - train lose: 6811.423905222039 - test lose: 19646.3125\n",
      "epoch 329 - train lose: 6801.099660773027 - test lose: 19705.6767578125\n",
      "epoch 330 - train lose: 6803.041940789473 - test lose: 19655.459106445312\n",
      "epoch 331 - train lose: 6801.244783100329 - test lose: 19654.367919921875\n",
      "epoch 332 - train lose: 6803.530119243421 - test lose: 19638.125366210938\n",
      "epoch 333 - train lose: 6790.924676192434 - test lose: 19707.987426757812\n",
      "epoch 334 - train lose: 6795.3497121710525 - test lose: 19660.018310546875\n",
      "epoch 335 - train lose: 6794.2823293585525 - test lose: 19582.251708984375\n",
      "epoch 336 - train lose: 6798.2946134868425 - test lose: 19767.0146484375\n",
      "epoch 337 - train lose: 6799.4251130756575 - test lose: 19695.73486328125\n",
      "epoch 338 - train lose: 6800.985094572368 - test lose: 19617.764526367188\n",
      "epoch 339 - train lose: 6799.390779194079 - test lose: 19791.856567382812\n",
      "epoch 340 - train lose: 6801.197831003289 - test lose: 19659.157348632812\n",
      "epoch 341 - train lose: 6800.782869037829 - test lose: 19657.720703125\n",
      "epoch 342 - train lose: 6795.459344161184 - test lose: 19806.49365234375\n",
      "epoch 343 - train lose: 6799.134225945723 - test lose: 19640.96142578125\n",
      "epoch 344 - train lose: 6785.271741365132 - test lose: 19601.58154296875\n",
      "epoch 345 - train lose: 6796.4269377055925 - test lose: 19612.025390625\n",
      "epoch 346 - train lose: 6795.928582442434 - test lose: 19530.602172851562\n",
      "epoch 347 - train lose: 6793.05126953125 - test lose: 19683.388916015625\n",
      "epoch 348 - train lose: 6790.2574270148025 - test lose: 19582.63037109375\n",
      "epoch 349 - train lose: 6789.3265573601975 - test lose: 19496.504638671875\n",
      "epoch 350 - train lose: 6789.797620271382 - test lose: 19569.992431640625\n",
      "epoch 351 - train lose: 6782.075889185855 - test lose: 19471.35693359375\n",
      "epoch 352 - train lose: 6784.606008429277 - test lose: 19568.135131835938\n",
      "epoch 353 - train lose: 6795.803376850329 - test lose: 19665.3193359375\n",
      "epoch 354 - train lose: 6805.62548828125 - test lose: 19635.76806640625\n",
      "epoch 355 - train lose: 6779.530479029605 - test lose: 19557.903198242188\n",
      "epoch 356 - train lose: 6789.650519120066 - test lose: 19637.626953125\n",
      "epoch 357 - train lose: 6786.122353001645 - test lose: 19640.7119140625\n",
      "epoch 358 - train lose: 6791.870888157895 - test lose: 19663.287841796875\n",
      "epoch 359 - train lose: 6793.80224609375 - test lose: 19500.5068359375\n",
      "epoch 360 - train lose: 6798.655067845395 - test lose: 19546.65087890625\n",
      "epoch 361 - train lose: 6782.8160207648025 - test lose: 19529.331787109375\n",
      "epoch 362 - train lose: 6786.3439298930925 - test lose: 19564.978149414062\n",
      "epoch 363 - train lose: 6789.991365131579 - test lose: 19664.680419921875\n",
      "epoch 364 - train lose: 6786.5009765625 - test lose: 19668.110595703125\n",
      "epoch 365 - train lose: 6777.843338815789 - test lose: 19633.236083984375\n",
      "epoch 366 - train lose: 6789.2674753289475 - test lose: 19523.915283203125\n",
      "epoch 367 - train lose: 6781.9412263569075 - test lose: 19537.217041015625\n",
      "epoch 368 - train lose: 6792.786209909539 - test lose: 19562.518432617188\n",
      "epoch 369 - train lose: 6788.926629317434 - test lose: 19497.66650390625\n",
      "epoch 370 - train lose: 6788.514494243421 - test lose: 19456.931884765625\n",
      "epoch 371 - train lose: 6781.689967105263 - test lose: 19411.595703125\n",
      "epoch 372 - train lose: 6786.247918379934 - test lose: 19550.996459960938\n",
      "epoch 373 - train lose: 6770.076171875 - test lose: 19581.306030273438\n",
      "epoch 374 - train lose: 6775.828536184211 - test lose: 19448.557861328125\n",
      "epoch 375 - train lose: 6774.6578176398025 - test lose: 19553.912109375\n",
      "epoch 376 - train lose: 6790.67333984375 - test lose: 19551.42724609375\n",
      "epoch 377 - train lose: 6772.962864925987 - test lose: 19511.1259765625\n",
      "epoch 378 - train lose: 6761.573447779605 - test lose: 19536.062744140625\n",
      "epoch 379 - train lose: 6767.860557154605 - test lose: 19546.404418945312\n",
      "epoch 380 - train lose: 6779.446134868421 - test lose: 19437.773315429688\n",
      "epoch 381 - train lose: 6770.187628495066 - test lose: 19359.628295898438\n",
      "epoch 382 - train lose: 6778.213815789473 - test lose: 19464.7412109375\n",
      "epoch 383 - train lose: 6778.2409025493425 - test lose: 19454.601806640625\n",
      "epoch 384 - train lose: 6768.613332648027 - test lose: 19510.908081054688\n",
      "epoch 385 - train lose: 6782.1060598273025 - test lose: 19428.090576171875\n",
      "epoch 386 - train lose: 6773.735274465461 - test lose: 19457.53125\n",
      "epoch 387 - train lose: 6771.7450143914475 - test lose: 19456.720458984375\n",
      "epoch 388 - train lose: 6768.726305509868 - test lose: 19460.010864257812\n",
      "epoch 389 - train lose: 6752.008583470395 - test lose: 19503.817626953125\n",
      "epoch 390 - train lose: 6768.040681537829 - test lose: 19441.123291015625\n",
      "epoch 391 - train lose: 6763.9718595805925 - test lose: 19484.78662109375\n",
      "epoch 392 - train lose: 6772.145713404605 - test lose: 19462.604248046875\n",
      "epoch 393 - train lose: 6758.130833675987 - test lose: 19485.4599609375\n",
      "epoch 394 - train lose: 6772.666555304277 - test lose: 19435.7236328125\n",
      "epoch 395 - train lose: 6768.41943359375 - test lose: 19611.8095703125\n",
      "epoch 396 - train lose: 6758.593955592105 - test lose: 19492.102416992188\n",
      "epoch 397 - train lose: 6766.713250411184 - test lose: 19413.907836914062\n",
      "epoch 398 - train lose: 6758.015111019737 - test lose: 19500.923583984375\n",
      "epoch 399 - train lose: 6768.9745065789475 - test lose: 19385.260864257812\n",
      "epoch 400 - train lose: 6761.290270353618 - test lose: 19416.562866210938\n",
      "epoch 401 - train lose: 6761.355982730263 - test lose: 19370.796630859375\n",
      "epoch 402 - train lose: 6754.709112870066 - test lose: 19548.875366210938\n",
      "epoch 403 - train lose: 6760.242315995066 - test lose: 19329.751220703125\n",
      "epoch 404 - train lose: 6752.163497121711 - test lose: 19363.074462890625\n",
      "epoch 405 - train lose: 6765.5902292351975 - test lose: 19418.426513671875\n",
      "epoch 406 - train lose: 6757.163548519737 - test lose: 19356.119140625\n",
      "epoch 407 - train lose: 6746.363409745066 - test lose: 19341.279296875\n",
      "epoch 408 - train lose: 6747.449141652961 - test lose: 19342.769409179688\n",
      "epoch 409 - train lose: 6769.839612458882 - test lose: 19306.387451171875\n",
      "epoch 410 - train lose: 6753.298545435855 - test lose: 19260.269287109375\n",
      "epoch 411 - train lose: 6753.972913240132 - test lose: 19296.88525390625\n",
      "epoch 412 - train lose: 6759.6627775493425 - test lose: 19323.631591796875\n",
      "epoch 413 - train lose: 6747.9207185444075 - test lose: 19247.93896484375\n",
      "epoch 414 - train lose: 6767.111456620066 - test lose: 19391.9365234375\n",
      "epoch 415 - train lose: 6766.802785773027 - test lose: 19321.818969726562\n",
      "epoch 416 - train lose: 6752.581285978618 - test lose: 19290.054077148438\n",
      "epoch 417 - train lose: 6760.563450863487 - test lose: 19407.731079101562\n",
      "epoch 418 - train lose: 6746.1094777960525 - test lose: 19412.31494140625\n",
      "epoch 419 - train lose: 6753.390959087171 - test lose: 19287.11669921875\n",
      "epoch 420 - train lose: 6754.8714792351975 - test lose: 19410.94970703125\n",
      "epoch 421 - train lose: 6737.223478618421 - test lose: 19437.024169921875\n",
      "epoch 422 - train lose: 6744.086271587171 - test lose: 19394.83984375\n",
      "epoch 423 - train lose: 6751.354209498355 - test lose: 19254.579467773438\n",
      "epoch 424 - train lose: 6754.209395559211 - test lose: 19433.765747070312\n",
      "epoch 425 - train lose: 6745.654759457237 - test lose: 19320.527099609375\n",
      "epoch 426 - train lose: 6743.155504728618 - test lose: 19239.8916015625\n",
      "epoch 427 - train lose: 6743.975174753289 - test lose: 19375.441284179688\n",
      "epoch 428 - train lose: 6739.077097039473 - test lose: 19352.414184570312\n",
      "epoch 429 - train lose: 6750.303068462171 - test lose: 19348.577026367188\n",
      "epoch 430 - train lose: 6745.9032175164475 - test lose: 19313.773681640625\n",
      "epoch 431 - train lose: 6742.2953073601975 - test lose: 19389.664794921875\n",
      "epoch 432 - train lose: 6747.719006990132 - test lose: 19395.640869140625\n",
      "epoch 433 - train lose: 6758.8621247944075 - test lose: 19378.463623046875\n",
      "epoch 434 - train lose: 6753.085860402961 - test lose: 19430.34326171875\n",
      "epoch 435 - train lose: 6746.1744962993425 - test lose: 19352.277587890625\n",
      "epoch 436 - train lose: 6746.295692845395 - test lose: 19078.64794921875\n",
      "epoch 437 - train lose: 6745.842644942434 - test lose: 19332.575439453125\n",
      "epoch 438 - train lose: 6757.6740080180925 - test lose: 19278.880737304688\n",
      "epoch 439 - train lose: 6747.726716694079 - test lose: 19432.443481445312\n",
      "epoch 440 - train lose: 6735.314735814145 - test lose: 19330.72412109375\n",
      "epoch 441 - train lose: 6750.996659128289 - test lose: 19371.654541015625\n",
      "epoch 442 - train lose: 6748.491133840461 - test lose: 19298.51611328125\n",
      "epoch 443 - train lose: 6751.538934004934 - test lose: 19334.628540039062\n",
      "epoch 444 - train lose: 6753.478592722039 - test lose: 19249.544799804688\n",
      "epoch 445 - train lose: 6730.6102487664475 - test lose: 19226.965454101562\n",
      "epoch 446 - train lose: 6733.190763774671 - test lose: 19400.681396484375\n",
      "epoch 447 - train lose: 6731.726511101973 - test lose: 19500.919189453125\n",
      "epoch 448 - train lose: 6746.705797697368 - test lose: 19395.551635742188\n",
      "epoch 449 - train lose: 6749.291529605263 - test lose: 19363.65478515625\n",
      "epoch 450 - train lose: 6729.8831208881575 - test lose: 19405.9208984375\n",
      "epoch 451 - train lose: 6738.061240748355 - test lose: 19341.2158203125\n",
      "epoch 452 - train lose: 6750.813142475329 - test lose: 19422.4375\n",
      "epoch 453 - train lose: 6746.463610197368 - test lose: 19286.59716796875\n",
      "epoch 454 - train lose: 6747.200452302632 - test lose: 19376.958862304688\n",
      "epoch 455 - train lose: 6743.414961965461 - test lose: 19310.375610351562\n",
      "epoch 456 - train lose: 6748.1844675164475 - test lose: 19311.18896484375\n",
      "epoch 457 - train lose: 6735.07958984375 - test lose: 19288.359375\n",
      "epoch 458 - train lose: 6739.148180509868 - test lose: 19240.917846679688\n",
      "epoch 459 - train lose: 6734.785567434211 - test lose: 19264.205932617188\n",
      "epoch 460 - train lose: 6735.028962787829 - test lose: 19324.4052734375\n",
      "epoch 461 - train lose: 6738.99755859375 - test lose: 19181.912719726562\n",
      "epoch 462 - train lose: 6731.638132195723 - test lose: 19210.59619140625\n",
      "epoch 463 - train lose: 6738.544767680921 - test lose: 19374.56689453125\n",
      "epoch 464 - train lose: 6738.5950349506575 - test lose: 19272.1328125\n",
      "epoch 465 - train lose: 6729.078125 - test lose: 19159.36669921875\n",
      "epoch 466 - train lose: 6742.138312088816 - test lose: 19213.375244140625\n",
      "epoch 467 - train lose: 6749.478310032895 - test lose: 19138.771484375\n",
      "epoch 468 - train lose: 6731.2877004523025 - test lose: 19109.0205078125\n",
      "epoch 469 - train lose: 6744.365876850329 - test lose: 19118.590087890625\n",
      "epoch 470 - train lose: 6743.1691252055925 - test lose: 19297.237915039062\n",
      "epoch 471 - train lose: 6739.246659128289 - test lose: 19286.456298828125\n",
      "epoch 472 - train lose: 6734.41162109375 - test lose: 19311.197998046875\n",
      "epoch 473 - train lose: 6743.189427425987 - test lose: 19268.70068359375\n",
      "epoch 474 - train lose: 6729.559056332237 - test lose: 19222.823486328125\n",
      "epoch 475 - train lose: 6734.377081620066 - test lose: 19298.08251953125\n",
      "epoch 476 - train lose: 6729.117161800987 - test lose: 19227.9619140625\n",
      "epoch 477 - train lose: 6733.786826685855 - test lose: 19375.2939453125\n",
      "epoch 478 - train lose: 6734.60400390625 - test lose: 19415.59521484375\n",
      "epoch 479 - train lose: 6739.028166118421 - test lose: 19301.60302734375\n",
      "epoch 480 - train lose: 6735.711991159539 - test lose: 19298.081298828125\n",
      "epoch 481 - train lose: 6730.123612253289 - test lose: 19234.381958007812\n",
      "epoch 482 - train lose: 6736.015368009868 - test lose: 19188.329833984375\n",
      "epoch 483 - train lose: 6728.50830078125 - test lose: 19121.35400390625\n",
      "epoch 484 - train lose: 6730.807848478618 - test lose: 19127.15283203125\n",
      "epoch 485 - train lose: 6728.3165090460525 - test lose: 19186.259521484375\n",
      "epoch 486 - train lose: 6725.886076274671 - test lose: 19240.673828125\n",
      "epoch 487 - train lose: 6739.929481907895 - test lose: 19206.8017578125\n",
      "epoch 488 - train lose: 6733.747198807566 - test lose: 19189.85498046875\n",
      "epoch 489 - train lose: 6733.4111328125 - test lose: 19410.856689453125\n",
      "epoch 490 - train lose: 6724.478489925987 - test lose: 19258.829833984375\n",
      "epoch 491 - train lose: 6732.60302734375 - test lose: 19280.978393554688\n",
      "epoch 492 - train lose: 6741.372430098684 - test lose: 19269.603515625\n",
      "epoch 493 - train lose: 6714.939376027961 - test lose: 19288.223388671875\n",
      "epoch 494 - train lose: 6726.535618832237 - test lose: 19328.17919921875\n",
      "epoch 495 - train lose: 6732.3705026726975 - test lose: 19189.3134765625\n",
      "epoch 496 - train lose: 6730.2571443256575 - test lose: 19250.48974609375\n",
      "epoch 497 - train lose: 6720.6436574835525 - test lose: 19086.356201171875\n",
      "epoch 498 - train lose: 6722.900827508223 - test lose: 19101.469482421875\n",
      "epoch 499 - train lose: 6720.960038034539 - test lose: 19220.303588867188\n"
     ]
    }
   ],
   "source": [
    "vae = VariationalAutoencoder(latent_dims).to(device) # GPU\n",
    "opt = torch.optim.Adam(vae.parameters())\n",
    "epochs = 500\n",
    "train_losses, test_losses = [],[]\n",
    "for i in range(epochs):\n",
    "    train_loss = train(train_loader, vae, opt)\n",
    "    train_losses.append(train_loss)\n",
    "    test_loss = valid(test_loader, vae)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"epoch {i} - train lose: {train_loss} - test lose: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d65e5740-f638-4068-9699-4d23db067e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iteration = np.arange(0, epochs, 1)\n",
    "\n",
    "plt.plot(iteration,train_losses,label=\"train loss\")\n",
    "plt.plot(iteration,test_losses,label=\"test loss\")\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a0d6e-3dc9-4d7b-9980-4a5fcf83344b",
   "metadata": {},
   "source": [
    "#### Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "36498be3-cc4b-423d-9af9-70ef245b756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(train_df.values.astype(np.float32)).to(device)\n",
    "test_tensor = torch.tensor(test_df.values.astype(np.float32)).to(device)\n",
    "\n",
    "train_result = vae.encoder(train_tensor)\n",
    "test_result = vae.encoder(test_tensor)\n",
    "\n",
    "train_result = pd.DataFrame(train_result.cpu().detach().numpy())\n",
    "test_result = pd.DataFrame(test_result.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2c2a31c5-0db8-4a3c-b9e9-9da4df423afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one-hot encoding for each stock\n",
    "encode = np.array([0,1,2,3,4,5])\n",
    "encode = np.repeat(encode,400)\n",
    "train_result['encode'] = encode\n",
    "\n",
    "encode = np.array([0,1,2,3,4,5])\n",
    "encode = np.repeat(encode,85)\n",
    "test_result['encode'] = encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d5c9386b-e2d1-42b9-8228-c1df5567f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ['TSLA', 'AAPL', 'AMZN', 'GOOG','NFLX', 'FB']\n",
    "X_train, y_train, X_test, y_test = [],[],[],[]\n",
    "sc_params = []\n",
    "for idx,symbol in enumerate(symbols):\n",
    "    # train/test set for each stock\n",
    "    train = train_result.iloc[400*idx:400*(idx+1)]\n",
    "    test = test_result.iloc[85*idx:85*(idx+1)]\n",
    "    \n",
    "    # origin_df contains original EOD features\n",
    "    origin_df = pd.read_csv('C:/Users/KingO/JupyterProject/stock-market-prediction/data/eod/{}.csv'.format(symbol))\n",
    "    # standardize origin_df\n",
    "    sc = StandardScaler()\n",
    "    origin_train_df = sc.fit_transform(origin_df.loc[0:399,['Open','High','Low','Close']])\n",
    "    origin_train_df = pd.DataFrame(origin_train_df, columns=['Open','High','Low','Close'])\n",
    "    origin_test_df = sc.transform(origin_df.loc[400:,['Open','High','Low','Close']])\n",
    "    origin_test_df = pd.DataFrame(origin_test_df, columns=['Open','High','Low','Close'])\n",
    "   \n",
    "    sc_params.append([sc.mean_,sc.var_])\n",
    "    \n",
    "    # create time_series training set\n",
    "    for i in range(len(train)-5):\n",
    "        X_train.append(train.iloc[i:i+5].to_numpy())\n",
    "        y_train.append(origin_train_df.loc[i+5,['Open','High','Low','Close']].to_numpy())\n",
    "    \n",
    "    # create time_series test set\n",
    "    for i in range(len(test)-5):\n",
    "        X_test.append(test.iloc[i:i+5].to_numpy())\n",
    "        y_test.append(origin_test_df.loc[i+5,['Open','High','Low','Close']].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f90b09e9-e1c5-48c5-aed8-02b51cb160bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('C:/Users/KingO/JupyterProject/stock-market-prediction/data/vae_features/X_train.npy', X_train)\n",
    "np.save('C:/Users/KingO/JupyterProject/stock-market-prediction/data/vae_features/y_train.npy', y_train)\n",
    "np.save('C:/Users/KingO/JupyterProject/stock-market-prediction/data/vae_features/X_test.npy', X_test)\n",
    "np.save('C:/Users/KingO/JupyterProject/stock-market-prediction/data/vae_features/y_test.npy', y_test)\n",
    "np.save('C:/Users/KingO/JupyterProject/stock-market-prediction/data/vae_features/sc_params.npy', sc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b75cc-0110-4b6d-a1aa-1c4ac3983763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
